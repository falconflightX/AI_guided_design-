{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ks_2samp\n",
    "from numpy.random import randn\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Embedding, Activation\n",
    "from keras.layers import concatenate, LeakyReLU, ReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,cross_val_score,cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "import scipy.stats\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # load data\n",
    "data_path = \"../Data/\"\n",
    "file_name = \"Table.xlsx\"\n",
    "\n",
    "df_main = pd.read_excel(data_path + file_name, skiprows=[0])\n",
    "df_main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main= df_main[['Cell_Energy',\n",
    " 'T_Trigger',\n",
    " 'P_Max',\n",
    " 'Delta_T_Ramp',\n",
    " 'Power_Plate',\n",
    " 'HeatingTime_Plate',\n",
    " 'Rth_JR_Shell',\n",
    " 'Lambda_20',\n",
    " 'Lambda_200',\n",
    " 'Lambda_400',\n",
    " 'Lambda_600',\n",
    " 'Lambda_800',\n",
    " 'Time_Trigger_T1F',\n",
    " 'Time_Trigger_T2F',\n",
    " 'Tmax T1F',\n",
    " 'Tmax T2F',\n",
    " 'Tmax Plateau T2F',\n",
    " ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_main.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\n",
    " 'Cell_Energy',\n",
    " 'T_Trigger',\n",
    " 'P_Max',\n",
    " 'Delta_T_Ramp',\n",
    " 'Power_Plate',\n",
    " 'HeatingTime_Plate',\n",
    " 'Rth_JR_Shell',\n",
    " 'Lambda_20',\n",
    " 'Lambda_200',\n",
    " 'Lambda_400',\n",
    " 'Lambda_600',\n",
    " 'Lambda_800']]\n",
    "\n",
    "y = df[[\n",
    " 'Time_Trigger_T1F',\n",
    " 'Time_Trigger_T2F',\n",
    " 'Tmax T1F',\n",
    " 'Tmax T2F',\n",
    " 'Tmax Plateau T2F',\n",
    " ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.boxplot(data=X)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[[i for i in X if X[i].nunique()>1]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to scale for DL model and other models - was not needed for RF (## To check)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "xscaler = StandardScaler()\n",
    "X_train_scaled = xscaler.fit_transform(X_train)\n",
    "X_test_scaled = xscaler.transform(X_test)\n",
    "\n",
    "yscaler = StandardScaler()\n",
    "y_train_scaled = yscaler.fit_transform(y_train)\n",
    "y_test_scaled = yscaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.boxplot(data=X_train_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "cb = MultiOutputRegressor(CatBoostRegressor())\n",
    "cb.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "# evaluate model on validation set\n",
    "y_pred_cb = cb.predict(X_test_scaled)\n",
    "y_ans_cb = yscaler.inverse_transform(y_pred_cb)\n",
    "pred_output_cb = pd.DataFrame(y_ans_cb, columns = y_test.columns)\n",
    "\n",
    "cols = pred_output_cb.columns.intersection(y_test.columns)\n",
    "# Iterate, Calculate, and Collect R-Squared Values\n",
    "r_squared = {c: scipy.stats.linregress(x=pred_output_cb[c], y=y_test[c]).rvalue ** 2\n",
    "             for c in cols}\n",
    "\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test, y_ans_cb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Optimization (Bayesian/TPE) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Input every row of \"y\" values from y_test as inputs to the optimization model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "# Define a function that takes inputs and returns the predicted outputs using the meta model\n",
    "def predict_outputs(x):\n",
    "    x = np.array(x).reshape(1, -1)  # convert input tuple to 2D array\n",
    "    #x_stacked = np.hstack(predict(x))  # stack the multioutput predictions horizontally\n",
    "    y_pred = cb.predict(x)  # make prediction using the meta-model\n",
    "    return y_pred.flatten()\n",
    "\n",
    "# Define a function that takes the inputs and the desired output values, and returns the negative of the objective function\n",
    "def objective_function(x, targets):\n",
    "    y_pred = predict_outputs(x)\n",
    "    error = np.sum(np.square(y_pred - targets))\n",
    "    return {'loss': error, 'status': STATUS_OK}\n",
    "\n",
    "target_outputs_scaled = y_test_scaled[:5,:] # set the desired output values (scaled)...I have a number 5 here for the number of rows. Increase to total length of y_test if needed\n",
    "\n",
    "optimized_inputs = [] # store the optimized inputs for each desired output value\n",
    "\n",
    "for targets in target_outputs_scaled:\n",
    "\n",
    "    # Define the range of values for each input feature\n",
    "    feature_ranges = [(np.min(X_train_scaled[:, i]), np.max(X_train_scaled[:, i])) for i in range(X_train_scaled.shape[1])]\n",
    "\n",
    "    # Set the range of values for the inputs\n",
    "    space = [hp.uniform('x{}'.format(i), feature_ranges[i][0], feature_ranges[i][1]) for i in range(X_train_scaled.shape[1])]\n",
    "\n",
    "    # Define the kernel for the Gaussian process used in Bayesian optimization\n",
    "    kernel = RBF(length_scale=1.0, length_scale_bounds=(1e-1, 10.0))\n",
    "\n",
    "    # Fit a Gaussian process to the training data\n",
    "    model = GaussianProcessRegressor(kernel=kernel, random_state=0).fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "    # Use the fmin function from hyperopt to find the inputs that minimize the objective function\n",
    "    trials = Trials()\n",
    "    objective_fn = lambda x: objective_function(x, targets)\n",
    "    best = fmin(objective_fn, space, algo=tpe.suggest, max_evals=1000, trials=trials, early_stop_fn=no_progress_loss(200), verbose=True)\n",
    "\n",
    "    # Get the inputs that minimize the objective function and the corresponding predicted outputs\n",
    "    x_opt_scaled = [best['x{}'.format(i)] for i in range(X_train_scaled.shape[1])]\n",
    "    x_opt = xscaler.inverse_transform(np.array(x_opt_scaled).reshape(1, -1)).flatten()\n",
    "\n",
    "    # Store the optimized inputs for this desired output value\n",
    "    optimized_inputs.append(x_opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(optimized_inputs, columns=X_train.columns) \n",
    "df_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [X_test,df_pred]\n",
    "f_names = [\"true\", \"pred\"]\n",
    "df_merge = pd.concat(dfs, ignore_index=True)\n",
    "df_merge['C'] = np.repeat(f_names, list(map(len, dfs)))\n",
    "\n",
    "fig, axes = plt.subplots(nrows=4, ncols=3, figsize=(15, 7), sharex=False, sharey=False)\n",
    "axes = axes.ravel()  # array to 1D\n",
    "cols = df_merge.columns[:-1]  # create a list of dataframe columns to use\n",
    "\n",
    "for col, ax in zip(cols, axes):\n",
    "    data = df_merge\n",
    "    sns.kdeplot(data=data, x=col, hue='C', shade=True, common_norm=False, ax=ax)\n",
    "    ax.set(title=f'Distribution of Column: {col}', xlabel=None)\n",
    "    \n",
    "#fig.delaxes(axes[5])  # delete the empty subplot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "rmse_error=[]\n",
    "trial = X_test.iloc[:5]\n",
    "for column in df_pred.columns:\n",
    "    mae = np.sqrt(mean_absolute_error(df_pred[column], trial[column]))\n",
    "    rmse_error.append(mae)\n",
    "\n",
    "rmse_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97ae2c0ecca85e86aaf7c7e10195371b4ff5e6fc5edc41c9a326cebe10a9efce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
